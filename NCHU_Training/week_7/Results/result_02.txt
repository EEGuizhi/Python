# 2022/8/23 16:05

>> Using cuda device
>> Training...
x_train:
 tensor([[[[-0.1451, -0.0588,  0.3490,  ...,  0.3882,  0.3882,  0.3882],
          [-0.0824, -0.0980,  0.2863,  ...,  0.3882,  0.3882,  0.3882],
          [ 0.0275, -0.1451,  0.1765,  ...,  0.3804,  0.3882,  0.3882],
          ...,
          [ 0.4745,  0.4745,  0.4824,  ...,  0.3804,  0.4118,  0.4275],
          [ 0.4902,  0.4980,  0.5137,  ...,  0.3804,  0.4039,  0.4275],
          [ 0.5451,  0.5529,  0.5608,  ...,  0.3725,  0.4039,  0.4196]],

         [[ 0.0275,  0.1137,  0.5216,  ...,  0.5922,  0.5922,  0.5922],
          [ 0.0902,  0.0745,  0.4588,  ...,  0.5922,  0.5922,  0.5922],
          [ 0.2000,  0.0275,  0.3490,  ...,  0.5922,  0.6000,  0.6000],
          ...,
          [ 0.5686,  0.5686,  0.5765,  ...,  0.5294,  0.5608,  0.5765],
          [ 0.5686,  0.5765,  0.5922,  ...,  0.5294,  0.5529,  0.5765],
          [ 0.6078,  0.6157,  0.6235,  ...,  0.5216,  0.5529,  0.5686]],

         [[ 0.1137,  0.2000,  0.6078,  ...,  0.6941,  0.6941,  0.6941],
          [ 0.1765,  0.1608,  0.5451,  ...,  0.6941,  0.6941,  0.6941],
          [ 0.2863,  0.1137,  0.4353,  ...,  0.6784,  0.6863,  0.6863],
          ...,
          [ 0.6627,  0.6627,  0.6706,  ...,  0.5843,  0.6157,  0.6314],
          [ 0.6627,  0.6706,  0.6863,  ...,  0.5843,  0.6078,  0.6314],
          [ 0.6941,  0.7020,  0.7098,  ...,  0.5765,  0.6078,  0.6235]]],


        [[[-0.9216, -0.9216, -0.9137,  ..., -0.8980, -0.8902, -0.8824],
          [-0.9216, -0.9216, -0.9137,  ..., -0.8902, -0.8824, -0.8824],
          [-0.9137, -0.9216, -0.9137,  ..., -0.8824, -0.8745, -0.8745],
          ...,
          [-0.8980, -0.8980, -0.8980,  ..., -0.8588, -0.8824, -0.8745],
          [-0.8980, -0.8980, -0.8980,  ..., -0.8667, -0.8902, -0.8824],
          [-0.9137, -0.9137, -0.9059,  ..., -0.8667, -0.8824, -0.8824]],

         [[-0.9294, -0.9294, -0.9216,  ..., -0.8902, -0.8824, -0.8745],
          [-0.9294, -0.9294, -0.9216,  ..., -0.8824, -0.8745, -0.8745],
          [-0.9216, -0.9294, -0.9216,  ..., -0.8667, -0.8588, -0.8588],
          ...,
          [-0.9059, -0.9059, -0.9059,  ..., -0.8275, -0.8353, -0.8275],
          [-0.9059, -0.9059, -0.9059,  ..., -0.8353, -0.8431, -0.8353],
          [-0.9216, -0.9216, -0.9137,  ..., -0.8353, -0.8353, -0.8353]],

         [[-0.9608, -0.9608, -0.9529,  ..., -0.9294, -0.9216, -0.9137],
          [-0.9608, -0.9608, -0.9529,  ..., -0.9294, -0.9216, -0.9216],
          [-0.9608, -0.9686, -0.9529,  ..., -0.9529, -0.9451, -0.9451],
          ...,
          [-0.9451, -0.9451, -0.9451,  ..., -0.9373, -0.9451, -0.9216],
          [-0.9451, -0.9451, -0.9451,  ..., -0.9451, -0.9529, -0.9294],
          [-0.9608, -0.9608, -0.9529,  ..., -0.9451, -0.9294, -0.9294]]],


        [[[ 0.0588,  0.0588,  0.0667,  ...,  0.1529,  0.1373,  0.1529],
          [ 0.0588,  0.0588,  0.0667,  ...,  0.1294,  0.1216,  0.1373],
          [ 0.0510,  0.0588,  0.0667,  ...,  0.1216,  0.1137,  0.1373],
          ...,
          [ 0.0275,  0.0196,  0.0275,  ...,  0.1686,  0.1608,  0.1529],
          [ 0.0196,  0.0118,  0.0275,  ...,  0.1529,  0.1451,  0.1529],
          [ 0.0118,  0.0039,  0.0196,  ...,  0.1451,  0.1373,  0.1608]],

         [[ 0.0588,  0.0588,  0.0667,  ...,  0.1529,  0.1373,  0.1529],
          [ 0.0588,  0.0588,  0.0667,  ...,  0.1294,  0.1216,  0.1373],
          [ 0.0510,  0.0588,  0.0667,  ...,  0.1216,  0.1137,  0.1373],
          ...,
          [ 0.0275,  0.0196,  0.0275,  ...,  0.1686,  0.1608,  0.1529],
          [ 0.0196,  0.0118,  0.0275,  ...,  0.1529,  0.1451,  0.1529],
          [ 0.0118,  0.0039,  0.0196,  ...,  0.1451,  0.1373,  0.1608]],

         [[ 0.0588,  0.0588,  0.0667,  ...,  0.1529,  0.1373,  0.1529],
          [ 0.0588,  0.0588,  0.0667,  ...,  0.1294,  0.1216,  0.1373],
          [ 0.0510,  0.0588,  0.0667,  ...,  0.1216,  0.1137,  0.1373],
          ...,
          [ 0.0275,  0.0196,  0.0275,  ...,  0.1686,  0.1608,  0.1529],
          [ 0.0196,  0.0118,  0.0275,  ...,  0.1529,  0.1451,  0.1529],
          [ 0.0118,  0.0039,  0.0196,  ...,  0.1451,  0.1373,  0.1608]]],


        ...,


        [[[ 0.0902,  0.0902,  0.0824,  ..., -0.0745, -0.0745, -0.0745],
          [ 0.0980,  0.0902,  0.0902,  ..., -0.0745, -0.0745, -0.0745],
          [ 0.0980,  0.0980,  0.0980,  ..., -0.0667, -0.0667, -0.0667],
          ...,
          [ 0.9294,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
          [ 0.9608,  1.0000,  0.8980,  ...,  1.0000,  1.0000,  1.0000],
          [ 0.9451,  0.9686,  0.9059,  ...,  1.0000,  1.0000,  1.0000]],

         [[-0.2000, -0.2000, -0.2078,  ..., -0.2863, -0.2863, -0.2863],
          [-0.1922, -0.2000, -0.2000,  ..., -0.2863, -0.2863, -0.2863],
          [-0.1922, -0.1922, -0.1922,  ..., -0.2784, -0.2784, -0.2784],
          ...,
          [ 0.5686,  0.5608,  0.6078,  ...,  0.6078,  0.6078,  0.6078],
          [ 0.5922,  0.5843,  0.5451,  ...,  0.6078,  0.6078,  0.6078],
          [ 0.5608,  0.5451,  0.6000,  ...,  0.6078,  0.6078,  0.6078]],

         [[-0.5294, -0.5294, -0.5373,  ..., -0.5216, -0.5216, -0.5216],
          [-0.5216, -0.5294, -0.5294,  ..., -0.5216, -0.5216, -0.5216],
          [-0.5373, -0.5373, -0.5216,  ..., -0.5137, -0.5137, -0.5137],
          ...,
          [ 0.1843,  0.2078,  0.2941,  ...,  0.2235,  0.2235,  0.2235],
          [ 0.2471,  0.2392,  0.2392,  ...,  0.2235,  0.2235,  0.2235],
          [ 0.2392,  0.2000,  0.2941,  ...,  0.2235,  0.2235,  0.2235]]],


        [[[-0.5608, -0.5608, -0.5608,  ..., -0.4431, -0.4431, -0.4431],
          [-0.5608, -0.5608, -0.5608,  ..., -0.4431, -0.4431, -0.4431],
          [-0.5608, -0.5608, -0.5529,  ..., -0.4431, -0.4431, -0.4431],
          ...,
          [-0.4980, -0.5059, -0.5059,  ..., -0.4118, -0.4196, -0.4196],
          [-0.5216, -0.5216, -0.5059,  ..., -0.4118, -0.4196, -0.4196],
          [-0.5216, -0.5137, -0.5137,  ..., -0.4196, -0.4196, -0.4196]],

         [[-0.3569, -0.3569, -0.3569,  ..., -0.2000, -0.2000, -0.2000],
          [-0.3569, -0.3569, -0.3569,  ..., -0.2000, -0.2000, -0.2000],
          [-0.3569, -0.3569, -0.3490,  ..., -0.2000, -0.2000, -0.2000],
          ...,
          [-0.3725, -0.3804, -0.3804,  ..., -0.2000, -0.2078, -0.2078],
          [-0.4039, -0.4039, -0.4039,  ..., -0.2078, -0.2157, -0.2157],
          [-0.4196, -0.4118, -0.4118,  ..., -0.2157, -0.2157, -0.2157]],

         [[-0.8510, -0.8667, -0.8667,  ..., -0.8275, -0.8275, -0.8275],
          [-0.8667, -0.8667, -0.8667,  ..., -0.8275, -0.8275, -0.8275],
          [-0.8667, -0.8667, -0.8588,  ..., -0.8275, -0.8275, -0.8275],
          ...,
          [-0.7255, -0.7333, -0.7333,  ..., -0.9294, -0.9373, -0.9373],
          [-0.7412, -0.7412, -0.7412,  ..., -0.9216, -0.9294, -0.9294],
          [-0.7569, -0.7490, -0.7490,  ..., -0.9294, -0.9294, -0.9294]]],


        [[[-0.6078, -0.5765, -0.5216,  ..., -0.0196, -0.0667, -0.1373],
          [-0.5686, -0.5451, -0.4980,  ..., -0.0275, -0.0275, -0.0431],
          [-0.4745, -0.4824, -0.4588,  ..., -0.1686, -0.1608,  0.0902],
          ...,
          [-0.6157, -0.5843, -0.5686,  ..., -0.3020, -0.2392, -0.1922],
          [-0.5137, -0.5059, -0.5451,  ..., -0.3176, -0.2471, -0.1294],
          [-0.3176, -0.2314, -0.1843,  ..., -0.5059, -0.4039, -0.2392]],

         [[-0.4510, -0.4196, -0.3804,  ...,  0.1137,  0.0824,  0.0196],
          [-0.4118, -0.3882, -0.3412,  ...,  0.1451,  0.1608,  0.1373],
          [-0.3098, -0.3176, -0.2941,  ..., -0.0039,  0.0275,  0.2784],
          ...,
          [-0.5686, -0.5294, -0.4824,  ..., -0.3176, -0.2471, -0.2000],
          [-0.4667, -0.4510, -0.4588,  ..., -0.3255, -0.2549, -0.1373],
          [-0.2627, -0.1686, -0.0980,  ..., -0.5137, -0.4039, -0.2392]],

         [[-0.8510, -0.8196, -0.7725,  ..., -0.1686, -0.1529, -0.1922],
          [-0.8275, -0.8039, -0.7569,  ..., -0.1373, -0.0902, -0.0824],
          [-0.7569, -0.7647, -0.7333,  ..., -0.2627, -0.2235,  0.0275],
          ...,
          [-0.6471, -0.6706, -0.7176,  ..., -0.2078, -0.1216, -0.0745],
          [-0.6863, -0.7176, -0.7961,  ..., -0.2157, -0.1451, -0.0275],
          [-0.5922, -0.5529, -0.5373,  ..., -0.4039, -0.3098, -0.1451]]]])
outputs:
 tensor([113, 342, 153, 340, 205,  59,  66,  33,  37, 188,  44, 272, 364, 170,
        110, 127, 144, 246, 383, 204,  17,  99, 389, 190, 358, 129, 178,  74,
        246, 372, 313, 333,  31,  11,  14, 230, 222,  69, 104, 311, 227, 382,
        398, 372, 340, 372,  24, 263, 133,   6, 193,  95,  98, 330, 377, 395,
        275,  99, 394, 175, 270, 393, 199, 256])
Traceback (most recent call last):
  File "d:\學校相關\大學專題\專題前訓練\week_7\week7_4109061012.py", line 88, in <module>
    y_train = model(x_train)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl      
    return forward_call(*input, **kwargs)
  File "d:\學校相關\大學專題\專題前訓練\week_7\week7_4109061012.py", line 47, in forward
    x = self.net(x)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl      
    return forward_call(*input, **kwargs)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torchvision\models\vgg.py", line 66, in forward
    x = self.features(x)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl      
    return forward_call(*input, **kwargs)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\container.py", line 139, in forward       
    input = module(input)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl      
    return forward_call(*input, **kwargs)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\pooling.py", line 162, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_jit_internal.py", line 423, in fn
    return if_false(*args, **kwargs)
  File "C:\Users\danie\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py", line 782, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory.
Tried to allocate 98.00 MiB (GPU 0; 4.00 GiB total capacity; 3.41 GiB already allocated; 0 bytes free; 3.43 GiB reserved in total by PyTorch)
If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. 
See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
